{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scott/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "sys.path.append(\"viewFormer\")\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from viewFormer.utils import get_model_layers\n",
    "from viewFormer.visualize import outlier_heatmap, abs_outlier_tensor\n",
    "\n",
    "# check torch gpu availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load model and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of life? I am not sure if I have a clear answer to that question. I think that the meaning of life is to be happy, to enjoy the life that you have, to enjoy the people you are with, to enjoy the food that you eat, to enjoy the music that you listen to, to enjoy the movies that you watch, to enjoy the books that you read, to enjoy the places that you go, to enjoy the things that you do, to enjoy the experiences that you have,\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# load prompt\n",
    "prompt = \"What is the meaning of life?\"\n",
    "\n",
    "# generate text\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "outputs = model.generate(input_ids, max_new_tokens=100)\n",
    "\n",
    "# decode text\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Obtain Model's Activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from timm.data import create_transform\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "def build_dataset_CIFAR100(is_train, data_path):\n",
    "    transform = build_transform(is_train)\n",
    "    dataset = datasets.CIFAR100(data_path, train=is_train, transform=transform, download=True)\n",
    "    nb_classes = 100\n",
    "    return dataset, nb_classes\n",
    "\n",
    "def build_transform(is_train):\n",
    "    input_size = 224\n",
    "    eval_crop_ratio = 1.0\n",
    "\n",
    "    resize_im = input_size > 32\n",
    "    if is_train:\n",
    "        # this should always dispatch to transforms_imagenet_train\n",
    "        transform = create_transform(\n",
    "            input_size=input_size,\n",
    "            is_training=True,\n",
    "            color_jitter=0.3,\n",
    "            auto_augment='rand-m9-mstd0.5-inc1',\n",
    "            interpolation='bicubic',\n",
    "            re_prob=0.0,\n",
    "            re_mode='pixel',\n",
    "            re_count=1,\n",
    "        )\n",
    "        if not resize_im:\n",
    "            # replace RandomResizedCropAndInterpolation with\n",
    "            # RandomCrop\n",
    "            transform.transforms[0] = transforms.RandomCrop(\n",
    "                input_size, padding=4)\n",
    "        return transform\n",
    "\n",
    "    t = []\n",
    "    if resize_im:\n",
    "        size = int(input_size / eval_crop_ratio)\n",
    "        t.append(\n",
    "            transforms.Resize(size, interpolation=3),  # to maintain same ratio w.r.t. 224 images\n",
    "        )\n",
    "        t.append(transforms.CenterCrop(input_size))\n",
    "\n",
    "    t.append(transforms.ToTensor())\n",
    "    t.append(transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD))\n",
    "    return transforms.Compose(t)\n",
    "\n",
    "def prepare_data(batch_size):\n",
    "    train_set, nb_classes = build_dataset_CIFAR100(is_train=True, data_path='./data')\n",
    "    test_set, _ = build_dataset_CIFAR100(is_train=False, data_path='./data')\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    return train_loader, test_loader, nb_classes\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    # model.eval()\n",
    "    model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on the test images: {accuracy}%')\n",
    "    return accuracy\n",
    "\n",
    "def train_one_epoch(model, criterion, optimizer, data_loader, device):\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    for image, target in tqdm(data_loader):\n",
    "        cnt += 1\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from viewFormer.data import calibrate\n",
    "from viewFormer.hooks import HookHandler, get_absmax_act_func\n",
    "\n",
    "def selected_layers(block_idx):\n",
    "    return [f'blocks.{block_idx}.attn.qkv', f'blocks.{block_idx}.mlp.fc1', f'blocks.{block_idx}.mlp.fc2']\n",
    "\n",
    "batch_size = 1\n",
    "train_loader, test_loader, nb_classes = prepare_data(batch_size)\n",
    "\n",
    "handler = HookHandler()\n",
    "layer_outputs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrate: [  0/256]\tTime  1.027 ( 1.027)\n",
      "Calibrate: [ 10/256]\tTime  0.178 ( 0.258)\n",
      "Calibrate: [ 20/256]\tTime  0.171 ( 0.217)\n",
      "Calibrate: [ 30/256]\tTime  0.170 ( 0.202)\n",
      "Calibrate: [ 40/256]\tTime  0.167 ( 0.194)\n",
      "Calibrate: [ 50/256]\tTime  0.171 ( 0.189)\n",
      "Calibrate: [ 60/256]\tTime  0.169 ( 0.186)\n",
      "Calibrate: [ 70/256]\tTime  0.170 ( 0.184)\n",
      "Calibrate: [ 80/256]\tTime  0.168 ( 0.182)\n",
      "Calibrate: [ 90/256]\tTime  0.184 ( 0.181)\n",
      "Calibrate: [100/256]\tTime  0.171 ( 0.180)\n",
      "Calibrate: [110/256]\tTime  0.175 ( 0.179)\n",
      "Calibrate: [120/256]\tTime  0.203 ( 0.179)\n",
      "Calibrate: [130/256]\tTime  0.168 ( 0.179)\n",
      "Calibrate: [140/256]\tTime  0.172 ( 0.178)\n",
      "Calibrate: [150/256]\tTime  0.173 ( 0.178)\n",
      "Calibrate: [160/256]\tTime  0.168 ( 0.177)\n",
      "Calibrate: [170/256]\tTime  0.168 ( 0.177)\n",
      "Calibrate: [180/256]\tTime  0.167 ( 0.176)\n",
      "Calibrate: [190/256]\tTime  0.168 ( 0.176)\n",
      "Calibrate: [200/256]\tTime  0.170 ( 0.176)\n",
      "Calibrate: [210/256]\tTime  0.171 ( 0.175)\n",
      "Calibrate: [220/256]\tTime  0.168 ( 0.175)\n",
      "Calibrate: [230/256]\tTime  0.168 ( 0.175)\n",
      "Calibrate: [240/256]\tTime  0.177 ( 0.175)\n",
      "Calibrate: [250/256]\tTime  0.187 ( 0.175)\n"
     ]
    }
   ],
   "source": [
    "model_blocks_cnt = len(model.blocks)\n",
    "block_layer_cnt = len(selected_layers(0))\n",
    "\n",
    "for i in range(model_blocks_cnt):\n",
    "    sample_block = get_model_layers(model, match_names=selected_layers(i), match_types=['Linear'])\n",
    "    handler.create_hooks(sample_block, get_absmax_act_func, layer_outputs)\n",
    "\n",
    "with torch.autocast(device_type=\"cuda\"):\n",
    "    calibrate(model, list(train_loader)[:256])\n",
    "handler.remove_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualize the Activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation = layer_outputs['blocks.10.attn.qkv']\n",
    "activation = model.model.layers[10].self_attn.q_proj.weight\n",
    "\n",
    "\n",
    "\n",
    "# create figure\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "fig.suptitle('QKV Activation', fontsize=24)\n",
    "\n",
    "# plot heatmap\n",
    "# ax = fig.add_subplot(1, 2, 1)\n",
    "# outlier_heatmap(activation, kernel_size=1, cmap='bwr', ax=ax)\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "def abs_outlier_tensor(w, ax=None, *args, **kwargs):\n",
    "    if ax is None:\n",
    "        fig, self_ax = plt.subplots(subplot_kw={'projection': '3d'}, figsize=(10, 10))\n",
    "    ax = ax or self_ax\n",
    "\n",
    "    w = np.abs(w)\n",
    "    min_val, max_val = np.min(w), np.max(w)\n",
    "\n",
    "\n",
    "    # set z axis min value\n",
    "    ax.set_zlim(min_val, max_val)\n",
    "\n",
    "    # draw 3d surface plot with coolwarm color map\n",
    "    x = np.arange(w.shape[1])  # Dims\n",
    "    y = np.arange(w.shape[0])  # Seqlen\n",
    "    xpos, ypos = np.meshgrid(x, y, copy=False)\n",
    "    zpos = w\n",
    "    surf = ax.plot_surface(xpos, ypos, zpos, cmap='coolwarm', antialiased=False)\n",
    "\n",
    "    # # use tri-surface plot\n",
    "    # x = np.arange(w.shape[1])  # Dims\n",
    "    # y = np.arange(w.shape[0])  # Seqlen\n",
    "    # xpos, ypos = np.meshgrid(x, y, copy=False)\n",
    "    # xpos = xpos.flatten()\n",
    "    # ypos = ypos.flatten()\n",
    "    # surf = ax.plot_trisurf(xpos, ypos, w.flatten(), *args, **kwargs)\n",
    "\n",
    "    # set labels\n",
    "    ax.set_xlabel('Out Dims', fontsize=18, labelpad=10)\n",
    "    ax.set_ylabel('SeqLen', fontsize=18, labelpad=10)\n",
    "    ax.set_zlabel('Absolute Value', fontsize=18, labelpad=10)\n",
    "\n",
    "    # set axis font size\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # draw color bar, pad is the distance between color bar and plot\n",
    "    m = cm.ScalarMappable(**kwargs)\n",
    "    m.set_array(w)\n",
    "    ax.figure.colorbar(m, ax=ax, shrink=0.6, pad=0.1).ax.tick_params(labelsize=14)\n",
    "    \n",
    "    # rotate the plot\n",
    "    # ax.view_init(elev=20, azim=30)\n",
    "\n",
    "    # set box aspect\n",
    "    # w_h = w.shape[1] / w.shape[0]\n",
    "    # ax.set_box_aspect([1, 1, 1])\n",
    "\n",
    "    return surf\n",
    "\n",
    "# plot 3d tensor\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "abs_outlier_tensor(activation.cpu().detach().numpy(), cmap='coolwarm', ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
